# -*- coding: utf-8 -*-
"""webScraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nmSdh0NNkTtvrMfq7SLJrzQ88-XDi5Ic
"""

!pip install requests beautifulsoup4
!pip install requests bs4
!pip install selenium
!pip install lxml
!pip install pypdf2

"""# WEB SCRAPING"""

import requests
from bs4 import BeautifulSoup
import os

BASE_URL = 'https://www.gutenberg.org/ebooks/search/?query='
DOWNLOAD_BASE_URL = 'https://www.gutenberg.org'
DOWNLOAD_FOLDER = 'enlightMe'
TERMINOS_BUSQUEDA = ['parenting', 'parenthood', 'pregnancy', 'pregnant', 'childs', 'children',
    'kindergarden', 'early-childhood', 'child-rearing', 'infancy',
    'child-development', 'toddlerhood', 'motherhood', 'fatherhood',
    'adolescence', 'newborn', 'child-nutrition', 'childcare',
    'child-education', 'child-welfare', 'family-planning',
    'child-psychology', 'maternal-health', 'pediatrics', 'postnatal',
    'prenatal-care', 'infant-care', 'child-safety']

def descargar_libro(url, folder):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Buscamos el enlace de descarga en formato TXT
    for link in soup.find_all('a'):
        href = link.get('href')
        if href and '.pdf' in href:
            download_link = DOWNLOAD_BASE_URL + href
            response = requests.get(download_link)
            filename = os.path.join(folder, href.split('/')[-1])

            with open(filename, 'wb') as f:
                f.write(response.content)
            break

def buscar_y_descargar_libros():
    if not os.path.exists(DOWNLOAD_FOLDER):
        os.makedirs(DOWNLOAD_FOLDER)

    for termino in TERMINOS_BUSQUEDA:
        search_url = BASE_URL + termino
        response = requests.get(search_url)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Buscamos los enlaces a las páginas de detalles de los libros
        for link in soup.find_all('a', class_='link'):
            href = link.get('href')
            if href and '/ebooks/' in href:
                libro_url = DOWNLOAD_BASE_URL + href
                descargar_libro(libro_url, DOWNLOAD_FOLDER)

if __name__ == '__main__':
    buscar_y_descargar_libros()

import requests
from bs4 import BeautifulSoup
import os

BASE_URL = 'https://open.umn.edu/opentextbooks/subjects/'
DOWNLOAD_BASE_URL = 'https://open.umn.edu/opentextbooks/'
BTN_URL= 'https://open.umn.edu/'
DOWNLOAD_FOLDER = 'textbooks'
TERMINOS_BUSQUEDA = ['parenting', 'parenthood', 'pregnancy', 'pregnant ', 'childs', 'children', 'kindergarden', 'early-childhood']

def descargar_libro(url, folder):
    print(url)
    '''response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    for link in soup.find_all('a'):
        if(link.text=='PDF'):
          search_href= requests.get('href')
          filename = os.path.join(folder, search_href.split('/')[-1]) + '.pdf'

          with open(filename, 'wb') as f:
            f.write(search_href.content)
            break'''

def buscar_y_descargar_libros():
    print('hola')
    if not os.path.exists(DOWNLOAD_FOLDER):
        os.makedirs(DOWNLOAD_FOLDER)
    print('path')
    for termino in TERMINOS_BUSQUEDA:
        search_url = BASE_URL + termino
        response = requests.get(search_url)

        soup = BeautifulSoup(response.content, 'html.parser')
        print(soup)
        print('parser')
        for link in soup.find_all('a', class_='primary with-arrows'):
            href = link.get('href')
            print('findall')
            download_link = BTN_URL + href
            descargar_libro(download_link, DOWNLOAD_FOLDER)
            print('descargar')

if __name__ == '__main__':
    buscar_y_descargar_libros()

import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin

BASE_URL = 'https://open.umn.edu/opentextbooks/subjects/'
DOWNLOAD_FOLDER = 'OpenUMNBooks'

CATEGORIES = [
    'early-childhood'
]

def descargar_libro(url, folder):
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()

        filename = os.path.join(folder, url.split('/')[-2] + '.pdf')

        with open(filename, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

    except requests.RequestException as e:
        print(f"Error al descargar {url}. Error: {e}")

def buscar_y_descargar_libros(categories):
    if not os.path.exists(DOWNLOAD_FOLDER):
        os.makedirs(DOWNLOAD_FOLDER)

    for category in categories:
        category_url = urljoin(BASE_URL, category)
        print(f"Intentando acceder a la categoría: {category_url}")

        response = requests.get(category_url)

        if response.status_code != 200:
            print(f"No se pudo acceder a {category_url}. Código de estado: {response.status_code}")
            continue

        soup = BeautifulSoup(response.content, 'html.parser')

        book_links = soup.find_all('a', class_='title')
        print(f"Encontrados {len(book_links)} libros en la categoría {category}.")

        for link in book_links:
            book_detail_url = 'https://open.umn.edu/opentextbooks/textbooks/habilidades-perceptivas-mejorando-el-aprendizaje-remoto-en-estudiantes-de-5-anos
            print(f"Visitando libro: {book_detail_url}")

            #... (resto del código sin cambios)


            detail_response = requests.get(book_detail_url)
            detail_soup = BeautifulSoup(detail_response.content, 'html.parser')

            # Encuentra el enlace que lleva al formato PDF
            pdf_format_link = detail_soup.find('a', href=True, text='PDF')

            if pdf_format_link:
                pdf_page_url = urljoin(BASE_URL, pdf_format_link['href'])
                print(f"Visitando página de formato PDF: {pdf_page_url}")

                pdf_page_response = requests.get(pdf_page_url)
                pdf_page_soup = BeautifulSoup(pdf_page_response.content, 'html.parser')

                # Buscar el enlace de "Digital PDF" en la nueva página
                download_link = pdf_page_soup.find('a', href=True, text='Digital PDF')

                if download_link:
                    print(f"Descargando libro desde: {download_link['href']}")
                    descargar_libro(download_link['href'], DOWNLOAD_FOLDER)
                else:
                    print("No se encontró el enlace de descarga 'Digital PDF'.")
            else:
                print("No se encontró el enlace a la página de formato PDF.")




if __name__ == '__main__':
    buscar_y_descargar_libros(CATEGORIES)



"""# PDF TO TXT CONVERTER"""

import os
import PyPDF2

def pdf_to_txt(pdf_path):
    """Convierte un archivo PDF en un archivo TXT"""
    with open(pdf_path, 'rb') as pdf_file:
        reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
    return text

def convert_and_merge(folder_path):
    """Convierte todos los archivos PDF en una carpeta a TXT y los combina en un solo archivo"""
    all_texts = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.pdf'):
            pdf_path = os.path.join(folder_path, filename)
            extracted_text = pdf_to_txt(pdf_path)
            all_texts.append(extracted_text)

    with open(os.path.join(folder_path, "merged.txt"), "w") as output_file:
        for text in all_texts:
            output_file.write(text)

if __name__ == "__main__":
    folder_path = input("Por favor, introduce la ruta de la carpeta que contiene los archivos PDF: ")
    convert_and_merge(folder_path)
    print("Archivos convertidos y fusionados en 'merged.txt'.")

"""# PRUEBAS"""

import os
import requests
from bs4 import BeautifulSoup

BASE_URL = 'https://open.umn.edu/opentextbooks/subjects/'
DOWNLOAD_FOLDER = '2'
TERMINOS_BUSQUEDA = [
     'nutrition', 'secondary-education', 'religion', 'gender-sexuality-studies', 'nursing'
]

BASE_URL_DOMAIN = 'https://open.umn.edu/opentextbooks/'

def descargar_libro(url, folder):
    # Paso 1: Obtener el enlace a la página secundaria desde open.umn.edu
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    pdf_link = soup.find("a", string="PDF")

    if pdf_link:
        href = pdf_link.get('href')

        # Paso 2: Acceder a la página secundaria en pressbooks.pub
        sec_response = requests.get(href)
        sec_soup = BeautifulSoup(sec_response.content, 'html.parser')

        # Paso 3: Buscar el enlace de descarga directa del PDF
        pdf_download_link = sec_soup.find("li", class_="dropdown-item").find("a")
        if pdf_download_link:
            download_href = pdf_download_link.get('href')

            # Paso 4: Descargar el PDF
            pdf_response = requests.get(download_href)
            filename = os.path.join(folder, download_href.split('/')[-1])
            with open(filename, 'wb') as f:
                f.write(pdf_response.content)




def buscar_y_descargar_libros():
    if not os.path.exists(DOWNLOAD_FOLDER):
        os.makedirs(DOWNLOAD_FOLDER)

    for termino in TERMINOS_BUSQUEDA:
        search_url = BASE_URL + termino
        print(f"Buscando libros con el término: {termino}")
        response = requests.get(search_url)

        soup = BeautifulSoup(response.content, 'html.parser')

        # Buscar todos los libros en la página de resultados
        entries = soup.find_all('entry')
        print(f"Encontrados {len(entries)} libros con el término: {termino}")
        for entry in entries:
            link_tag = entry.find('link', rel="alternate")
            if link_tag:
                href = link_tag.get('href')
                descargar_libro(href, DOWNLOAD_FOLDER) # Directamente usar href sin concatenar
            else:
                print("No se encontró el enlace 'alternate' para este libro.")

if __name__ == '__main__':
    buscar_y_descargar_libros()